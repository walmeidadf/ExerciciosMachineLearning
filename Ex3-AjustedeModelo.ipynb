{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning your model\n",
    "\n",
    "Having trained your model, your next task is to evaluate its performance. What metrics can you use to gauge how good your model is? So far, you have used accuracy for classification and R-squared for regression. In this chapter, you will learn about some of the other metrics available in scikit-learn that will allow you to assess your model's performance in a more nuanced manner. You will then learn to optimize both your classification as well as regression models using hyperparameter tuning.\n",
    "\n",
    "## Classification metrics\n",
    "* Measuring model performance with accuracy:\n",
    "- Fraction of correctly classified samples\n",
    "- Not always a useful metric\n",
    "\n",
    "### Class imbalance example: Emails\n",
    "Consider a spam classification problem in which 99% of emails are real and only 1% are spam.\n",
    "I could build a model that classified all email as real: this model would be correct 99% of the time and thus have an accuracy of 99%, which sounds great. However, this naive classifier does a horrible job of predicting spam: it never predicts spam at all, so it completely fails at its original purpose.\n",
    "The situation when one class is more frequent os call **class imbalance**, because the class of real email contains way more instances than the class of spam.\n",
    "This is a very common situation in practice and requires a more nuanced metric to assess the performance of our model.\n",
    "\n",
    "### Diagnosing classification predictions\n",
    "\n",
    "Given a binary classifier, we can draw up a 2-by-2 matrix that summarizes predictive performance called a **confusion matrix**.\n",
    "\n",
    "Across the top are the predicted labels, down the side actual labels.\n",
    "\n",
    "<img src=\"confusion-matrix.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Given any model, we can fill in the confusion matrix according to its predictions.\n",
    "\n",
    "### Why do we care about the confusion matrix?\n",
    "\n",
    "First, notice that you can retrieve accuracy from the confusion matrix: it's the sum of the diagonal divided by the total sum of the matrix.\n",
    "\n",
    "There are several other important metrics you can easily calculate from the confusion matrix.\n",
    "* Precision: true positives / ( true positives + false positives ) . It is called the positive predictive value or PPV.\n",
    "* Recall: true positives / ( true positives + false negatives ) . This is called sensitivity, hit rate, or true positive rate.\n",
    "* F1 Score: 2 * (precision * recall) / (precision + recall). In other words, it's the harmonic mean of precision and recall.\n",
    "\n",
    "High precision: low false positive rate, that is, not many real emails were predicted as being spam.\n",
    "\n",
    "High recall means that our classifier predicted most positive or spam emails correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages and libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise - Metrics for classification\n",
    "\n",
    "Here, you'll work with the PIMA Indians dataset obtained from the UCI Machine Learning Repository. The goal is to predict whether or not a given female patient will contract diabetes based on features such as BMI, age, and number of pregnancies. Therefore, it is a binary classification problem. A target value of 0 indicates that the patient does not have diabetes, while a value of 1 indicates that the patient does have diabetes.\n",
    "\n",
    "Your job is to train a k-NN classifier to the data and evaluate its performance by generating a confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset into a DataFrame df and the feature and target variable arrays X and y \n",
    "df = pd.read_csv('diabetes.csv')\n",
    "X = df.drop('diabetes', axis = 1)\n",
    "y = df.diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[176  30]\n",
      " [ 56  46]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.85      0.80       206\n",
      "           1       0.61      0.45      0.52       102\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       308\n",
      "   macro avg       0.68      0.65      0.66       308\n",
      "weighted avg       0.71      0.72      0.71       308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Instantiate a k-NN classifier: knn\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test data: y_pred\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Generate the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression and the ROC curve\n",
    "\n",
    "Despite its name, logistic regression is used in classification problems, not regression problems.\n",
    "\n",
    "Logistic regression for binary classification\n",
    "* Given one feature, log reg will output a probability, p, with respect to the target variable.\n",
    "\n",
    "Probability thresholds\n",
    "* By default, logistic regression threshold = 0.5\n",
    "* Not specific to logistic regression\n",
    "- k-NN classifiers also have thresholds\n",
    "\n",
    "### The ROC curve\n",
    "\n",
    "What happens if we vary the threshold?\n",
    "In particular, what happens to the true positive and false positive rates as we vary the threshold?\n",
    "\n",
    "When the threshold equals zero, the models predicts '1' for all the data, which means the true positive is equal to the false positive rate is equal to one.\n",
    "\n",
    "When the threshold equals '1', the model predicts '0' for all the data, which means that both true and false positive rates are 0.\n",
    "\n",
    "If we vary the threshold between these two extremes, we get a series of different false positive and true positive rates.\n",
    "\n",
    "The set of point we get when trying all possible thresholds is called the Receiver Operating Characteristic curve or ROC curve.\n",
    "\n",
    "<img src=\"roc-curve.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[174  32]\n",
      " [ 36  66]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.84      0.84       206\n",
      "           1       0.67      0.65      0.66       102\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       308\n",
      "   macro avg       0.75      0.75      0.75       308\n",
      "weighted avg       0.78      0.78      0.78       308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise - Building a logistic regression model\n",
    "\n",
    "# Import the necessary modules\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)\n",
    "\n",
    "# Create the classifier: logreg\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Compute and print the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting an ROC curve\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Compute predicted probabilities: y_pred_prob\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-recall Curve\n",
    "\n",
    "When looking at your ROC curve, you may have noticed that the y-axis (True positive rate) is also known as recall. Indeed, in addition to the ROC curve, there are other ways to visually evaluate model performance. One such way is the precision-recall curve, which is generated by plotting the precision and recall for different thresholds. As a reminder, precision and recall are defined as:\n",
    "\n",
    "<img src=\"precision-recall-curve.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "### Area under the ROC curve (AUC)\n",
    "\n",
    "The question is: given the ROC curve, can we extract a metric of interest?\n",
    "\n",
    "The larger the area under the ROC curve, the better our model is!\n",
    "\n",
    "The way to think about this is the following: if we had a model which produced an ROC curve that had a single point at 1,0, the upper left corner, representing a true positive rate of one a a false positive rate of zero, this would be a great model!\n",
    "For this reason, the area under the ROC, commonly denoted as AUC, is another popular metric for classification models.\n",
    "\n",
    "<img src=\"roc-auc.png\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8268608414239482\n",
      "AUC scores computed using 5-fold cross-validation: [0.7987037  0.80777778 0.81944444 0.86622642 0.85056604]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Compute predicted probabilities: y_pred_prob\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute and print AUC score\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\n",
    "\n",
    "# Compute cross-validated AUC scores: cv_auc\n",
    "cv_auc = cross_val_score(logreg, X, y, cv=5, scoring='roc_auc')\n",
    "\n",
    "# Print list of AUC scores\n",
    "print(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "* Linear regression: Choosing parameters for the model that fit the data the best\n",
    "* Ridge/lasso regression: choosing alpha before fitting it\n",
    "* k-Nearest Neighbors: Choosing n_neighbors before fitting and predicting\n",
    "* Parameters like alpha and k, ones that need to be specified before fitting a model, are called Hyperparameters.\n",
    "\n",
    "These are parameters that cannot be explicitly learned by fitting the model.\n",
    "\n",
    "### Choosing the correct hyperparameter\n",
    "* Try a bunch of different hyperparameter values\n",
    "* Fit all of them separately\n",
    "* See how well each performs\n",
    "* Choose the best performing one\n",
    "\n",
    "This is called **hyperparameter tunning** and doing so in this fashion is the current standard.\n",
    "\n",
    "It is essential to use cross-validation, because as using train teste split alone would risk overfitting the hyperparameter to the test set.\n",
    "\n",
    "### Grid search cross-validation\n",
    "\n",
    "We choose a grid of possible values we want to try for the hyperparameter(s).\n",
    "\n",
    "For example, if we had two hyperparameters, C and alpha, the grid values to test could like the figure above. We then perform k-fold cross-validation for each point in the grid, that is, for each choice of hyperparameter or combination of hyperparameters. We then choose for our model the choose of hyperparamters that performed the best!\n",
    "\n",
    "<img src=\"grid-search.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "### *C* regularization parameter in logistic regression \n",
    "\n",
    "Like the alpha parameter of lasso and ridge regularization that you saw earlier, logistic regression also has a regularization parameter: C. C controls the inverse of the regularization strength, and this is what you will tune in this exercise. A large C can lead to an overfit model, while a small C can lead to an underfit model.\n",
    "\n",
    "#### Exercise - Hyperparameter tuning with GridSearchCV\n",
    "\n",
    "Your job is to use GridSearchCV and logistic regression to find the optimal C in this hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'C': 268.2695795279727}\n",
      "Best score is 0.7708333333333334\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Setup the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space}\n",
    "\n",
    "# Instantiate a logistic regression classifier: logreg\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "logreg_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning with RandomizedSearchCV\n",
    "\n",
    "GridSearchCV can be computationally expensive, especially if you are searching over a large hyperparameter space and dealing with multiple hyperparameters. A solution to this is to use RandomizedSearchCV, in which not all hyperparameter values are tried out. Instead, a fixed number of hyperparameter settings is sampled from specified probability distributions.\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "Just like k-NN, linear regression, and logistic regression, decision trees in scikit-learn have .fit() and .predict() methods that you can use in exactly the same way as before. Decision trees have many parameters that can be tuned, such as max_features, max_depth, and min_samples_leaf: This makes it an ideal use case for RandomizedSearchCV.\n",
    "\n",
    "#### Exercise\n",
    "As before, the feature array X and target variable array y of the diabetes dataset have been pre-loaded. The hyperparameter settings have been specified for you. Your goal is to use RandomizedSearchCV to find the optimal hyperparameters. Go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 6, 'min_samples_leaf': 2}\n",
      "Best score is 0.7473958333333334\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from scipy.stats import randint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "\n",
    "# Setup the parameters and distributions to sample from: param_dist\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": randint(1, 9),\n",
    "              \"min_samples_leaf\": randint(1, 9),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# Instantiate a Decision Tree classifier: tree\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Instantiate the RandomizedSearchCV object: tree_cv\n",
    "tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "tree_cv.fit(X,y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\n",
    "print(\"Best score is {}\".format(tree_cv.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold-out set for final evaluation\n",
    "\n",
    "Hold-out set reasoning:\n",
    "* How well can the model perform on never before seen data? SO, I want to use my model to predict on some labeled data, compare my prediction to the actual labels, and compute the scoring function.\n",
    "\n",
    "* Using ALL data for cross-validation is not ideal\n",
    "\n",
    "* Split data into training and hold-out set at the beginning\n",
    "\n",
    "* Perform grid search cross-validation on training set\n",
    "\n",
    "* Choose best hyperparamenters and evaluate on hold-out set\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "In addition to C, logistic regression has a 'penalty' hyperparameter which specifies whether to use 'l1' or 'l2' regularization. Your job in this exercise is to create a hold-out set, tune the 'C' and 'penalty' hyperparameters of a logistic regression classifier using GridSearchCV on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameter: {'C': 31.622776601683793, 'penalty': 'l2'}\n",
      "Tuned Logistic Regression Accuracy: 0.7673913043478261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space, 'penalty': ['l1', 'l2']}\n",
    "\n",
    "# Instantiate the logistic regression classifier: logreg\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the training data\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the optimal parameters and best score\n",
    "print(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv.best_params_))\n",
    "print(\"Tuned Logistic Regression Accuracy: {}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic net regularization\n",
    "\n",
    "Remember lasso and ridge regression from the previous chapter? Lasso used the L1 penalty to regularize, while ridge used the L2 penalty. There is another type of regularized regression known as the elastic net. In elastic net regularization, the penalty term is a linear combination of the L1 and L2 penalties:\n",
    "\n",
    "a∗L1+b∗L2\n",
    "\n",
    "In scikit-learn, this term is represented by the 'l1_ratio' parameter: An 'l1_ratio' of 1 corresponds to an L1 penalty, and anything lower is a combination of L1 and L2.\n",
    "\n",
    "#### Exercise\n",
    "In this exercise, you will GridSearchCV to tune the 'l1_ratio' of an elastic net model trained on the Gapminder data. As in the previous exercise, use a hold-out set to evaluate your model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned ElasticNet l1 ratio: {'l1_ratio': 0.0}\n",
      "Tuned ElasticNet R squared: 0.24765337510702734\n",
      "Tuned ElasticNet MSE: 0.16664179543611005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Create the hyperparameter grid\n",
    "l1_space = np.linspace(0, 1, 30)\n",
    "param_grid = {'l1_ratio': l1_space}\n",
    "\n",
    "# Instantiate the ElasticNet regressor: elastic_net\n",
    "elastic_net = ElasticNet()\n",
    "\n",
    "# Setup the GridSearchCV object: gm_cv\n",
    "gm_cv = GridSearchCV(elastic_net, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the training data\n",
    "gm_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set and compute metrics\n",
    "y_pred = gm_cv.predict(X_test)\n",
    "r2 = gm_cv.score(X_test, y_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Tuned ElasticNet l1 ratio: {}\".format(gm_cv.best_params_))\n",
    "print(\"Tuned ElasticNet R squared: {}\".format(r2))\n",
    "print(\"Tuned ElasticNet MSE: {}\".format(mse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
